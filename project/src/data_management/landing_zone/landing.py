# -*- coding: utf-8 -*-
"""1.landing_zone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UU1UFqN8ptmZ5i7DyHva9kBJyTGWnhrb

# Landing Zone

**Brief Description**:

Given that the landing zone is volatile, data will be extracted from the landing static zone and this script will perform two things in the form of an ETL task:

1. Raw data will be loaded and structured (NAME CONVENTION IS CRUCIAL FOR INGESTION: **SOURCE_SUBSOURCE_TIMESTAMP*.csv**)
2. Both data-sets from **portal_dades** will be splitted artificially in order to guarantee the constraint NÂº1 of the project.



---

**Input**: The input consists of the .csv sources file of the project which can be found at ADSDB/landing_zone/input

**Output**: The output is the manipulation-ready datasets, which can be found in the input of the following zone: ADSDB/formatted_zone/input

> MAIN OBJECTIVE: Data Ingestion

## Initial data loading (**Extract**)

Firstly the main libraries are imported and the drive filesystem is mounted so the files can be accessed inside the ADSDB/landing/temporal directory. ðŸ’¾
"""

### Main useful libraries: loguru, ruff, pydantic, pytest, poetry, tqdm, pathlib, argparse
import os
import sys
from datetime import datetime
import glob
import re
import shutil
from dataclasses import dataclass, field
from typing import Dict, List, Optional

!pip install loguru
import loguru

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import duckdb

from google.colab import drive
drive.mount("/content/drive/")


### GLOBAL VARIABLES
temporal_path = "drive/MyDrive/Assignatures/ADSDB/landing_zone/temporal"
persistent_path = "drive/MyDrive/Assignatures/ADSDB/landing_zone/persistent"
year_to_split = 2023
#This data should be parsed through a command line interface
files_to_split = ["portaldades_lloguerpreus_2024.csv", "portaldades_lloguerrecompte_2024.csv"]

"""> The input filenames follow a structured format like "source_subsource_year".csv, where "source" is the main data provider, "subsource" categorizes the data, and "year" indicates the dataset's year. If the file starts with artificial_, it signals the data is generated or modified. This rich metadata allows easy categorization and most importantly cohesion of dates (It wouldn't make sense that ) - A regex extracts this metadata automatically from filenames to help organize, timestamp, and process the files efficiently.



In order to guarantee the first constraint of the project (**Which states that is mandatory to have 2 temporal versions of the same data**), we are going to split both yearly rent datasets into two halves in order to simulate 2 data-batches.
"""

@dataclass
class FileInfo:
    filename: str
    filepath: str
    df: Optional[pd.DataFrame] = field(default=None)
    source: Optional[str] = field(default=None)
    sub_source: Optional[str] = field(default=None)
    year: Optional[int] = field(default=None)

    def __repr__(self):
        return (f"FileInfo(filename='{self.filename}', filepath='{self.filepath}', "
                f"df_shape={self.df.shape if self.df is not None else None}, "
                f"source='{self.source}', sub_source='{self.sub_source}', year={self.year})")


@dataclass
class FileInfoManager:
    directory: str
    files: Dict[str, FileInfo] = field(default_factory=dict)

    def __post_init__(self):
        self.files = self._load_csv_files(self.directory)

    def _load_csv_files(self, directory: str) -> Dict[str, FileInfo]:
        """
        Recursively load all CSV files from the directory and create FileInfo objects.
        """
        csv_file_paths = glob.glob(os.path.join(directory, '**', '*.csv'), recursive=True)
        file_info_dict = {}

        for path in csv_file_paths:
            filename = os.path.basename(path)
            file_info = self._create_file_info(filename, path)
            file_info_dict[filename] = file_info

        loguru.logger.info(f"Loaded {len(file_info_dict)} files from directory {directory}.")
        return file_info_dict

    def _create_file_info(self, filename: str, filepath: str, df: Optional[pd.DataFrame]=None) -> FileInfo:
        """
        Extracts source, sub-source, and year from the filename using regex.
        Example pattern: 'source_subsource_year.csv'
        """
        match = re.match(r'([^_]+)_([^_]+)_(\d{4})', filename)
        if match:
            source = match.group(1)
            sub_source = match.group(2)
            year = int(match.group(3))
        else:
            source = None
            sub_source = None
            year = None

        return FileInfo(filename=filename, filepath=filepath, source=source, sub_source=sub_source, year=year,df=df)

    def load_dataframe(self, filename: str) -> Optional[pd.DataFrame]:
        """
        Loads a DataFrame from a CSV file if not already loaded. Returns None if file doesn't exist.
        """
        file_info = self.files.get(filename)
        if file_info:
            if file_info.df is None:
                try:
                    file_info.df = pd.read_csv(file_info.filepath)
                    loguru.logger.success(f"DataFrame loaded for file: {filename}")
                except Exception as e:
                    loguru.logger.error(f"Failed to load DataFrame for {filename}: {e}")
                    return None
            return file_info.df
        else:
            loguru.logger.error(f"File '{filename}' not found in FileInfoManager.")
            return None

    def load_all_dataframes(self):
      for filename in self.files.keys():
        self.load_dataframe(filename)

    def get_file_info(self, filename: str) -> Optional[FileInfo]:
        """
        Retrieves the FileInfo object for a given filename.
        """
        return self.files.get(filename)

    def add_file(self, file_info: FileInfo):
        """
        Adds a new FileInfo object to the manager.
        """
        self.files[file_info.filename] = file_info
        loguru.logger.info(f"File '{file_info.filename}' added to FileInfoManager.")

    def add_file_by_path(self, filepath: str, df: Optional[pd.DataFrame] = None):
        """
        Adds a new FileInfo object to the manager by filepath.
        """
        filename = os.path.basename(filepath)
        self.files[filename] = self._create_file_info(filename, filepath, df)
        loguru.logger.info(f"File '{filename}' added to FileInfoManager.")

    def remove_file(self, filename: str):
        """
        Removes a file from the manager by filename.
        """
        if filename in self.files:
            del self.files[filename]
            loguru.logger.info(f"File '{filename}' removed from FileInfoManager.")
        else:
            loguru.logger.warning(f"Tried to remove '{filename}', but it was not found.")

    def update_filepaths(self, base_path: str):
      """
      Updates the file paths of each FileInfo in the FileInfoManager using the base path,
      source, sub-source, and year.

      Filepath format: base_path/source/sub_source/year.csv
      """
      for filename, file_info in self.files.items():
          if file_info.source and file_info.sub_source and file_info.year:
              # Construct the new file path
              new_filepath = os.path.join(base_path, file_info.source, file_info.sub_source, file_info.filename)

              # Update the FileInfo with the new path
              file_info.filepath = new_filepath
              loguru.logger.success(f"Updated {filename} -> {new_filepath}")
          else:
              loguru.logger.error(f"Skipping {filename}: source/sub_source/year information is missing")

    def save_all_files(self, base_path: str, add_timestamp: bool = False):
        """
        Saves all loaded DataFrames to the specified base path.
        The directory structure is 'base_path/source/sub_source/year.csv'.
        If add_timestamp is True, appends a timestamp to the filenames.
        """
        for file_info in self.files.values():
            if file_info.df is not None:
                # Create structured directory
                save_directory = os.path.join(base_path, file_info.source, file_info.sub_source)
                os.makedirs(save_directory, exist_ok=True)

                # Construct the filename with optional timestamp
                name, ext = os.path.splitext(file_info.filename)
                if add_timestamp:
                    timestamp = datetime.now().strftime('%Y%m%d')
                    name = f"{name}_{timestamp}"
                save_path = os.path.join(save_directory, f"{name}{ext}")

                try:
                    file_info.df.to_csv(save_path, index=False)
                    loguru.logger.success(f"File '{file_info.filename}' saved as '{name}' in '{save_directory}'")
                except Exception as e:
                    loguru.logger.error(f"Error saving '{file_info.filename}' to '{save_path}': {e}")
            else:
                loguru.logger.warning(f"File '{file_info.filename}' has no DataFrame loaded, skipping save.")

    def save_file(self, filename: str, base_path: str, add_timestamp: bool = False):
        """
        Saves a single file's DataFrame to the specified base path.
        The file will be saved as 'base_path/source/sub_source/year.csv'.
        If add_timestamp is True, appends a timestamp to the filename.
        """
        file_info = self.files.get(filename)
        if file_info and file_info.df is not None:
            save_directory = os.path.join(base_path, file_info.source, file_info.sub_source)
            os.makedirs(save_directory, exist_ok=True)

            # Construct the filename with optional timestamp
            name = f"{file_info.year}"  # Default: year.csv
            if add_timestamp:
                timestamp = datetime.now().strftime('%Y%m%d')
                name = f"{name}_{timestamp}.csv"
            else:
                name = f"{name}.csv"

            save_path = os.path.join(save_directory, name)

            try:
                file_info.df.to_csv(save_path, index=False)
                loguru.logger.success(f"File '{filename}' saved as '{name}' in '{save_directory}'")
            except Exception as e:
                loguru.logger.error(f"Error saving '{filename}' to '{save_path}': {e}")
        else:
            loguru.logger.warning(f"File '{filename}' not found or has no DataFrame loaded.")

    def list_files(self) -> List[str]:
        return list(self.files.keys())


# Example usage
file_manager = FileInfoManager(directory=temporal_path)
file_manager.list_files()

#Get a portaldades dataframe BUT GET IT PLEASE
file_manager.get_file_info("portaldades_lloguerpreus_2024.csv").df

"""## Splitting rent_prices & rent_count (**Transform**)

In this section both prices and count **.csv** fles are splitted into 2 versions: 2022 and 2024. In the actual data there are 2 kind of temporal patterns in which data is represented:

- Year
- Quarter of the year

This regex should be meticulously revised if the timestamp format changes.
"""

def split_dataframe_by_time(df, split_year):
    """
    Splits dataframe by comparing year or quarter-like columns to a split year,
    without modifying or standardizing the data. The split year is not included
    in the second dataframe.
    """
    year_columns = []
    quarter_columns = []

    for col in df.columns:
        # Detect yearly columns
        if re.match(r'^\d{4}$', col):
            year_columns.append(col)
        # Detect quarterly columns
        elif re.match(r'^\d{1}[nrt]\str\s\d{4}$', col):
            quarter_columns.append(col)

    # Split yearly columns based on the split year
    year_before = [col for col in year_columns if int(col) < split_year]
    year_after = [col for col in year_columns if int(col) >= split_year]

    # Split quarterly columns based on the split year
    quarter_before = [col for col in quarter_columns if int(col.split()[-1]) < split_year]
    quarter_after = [col for col in quarter_columns if int(col.split()[-1]) >= split_year]

    # Create two new dataframes: one for before the split year, one for after
    df_before = df[df.columns[:3].tolist() + year_before + quarter_before]  # Keep first 3 identifier columns
    df_after = df[df.columns[:3].tolist() + year_after + quarter_after]

    return df_before, df_after

def modify_filename(filename, split_year):
    """
    Modifies the filename by either replacing the year at the end or appending
    the split year otherwise.
    """
    # Separate the file name from the extension
    name, ext = os.path.splitext(filename)

    # Match a year (4 digits) at the end of the file name, before the extension
    match = re.search(r'_(\d{4})$', name)

    if match:
        # If a year is found, replace it with the split year
        new_name = re.sub(r'_(\d{4})$', f'_{split_year}', name)
    else:
        # If no year is found, append the split year before the extension
        new_name = f"{name}_{split_year}"

    # Reattach the file extension
    new_filename = f"{new_name}{ext}"

    return new_filename

def perform_data_splitting(temporal_data, file_manager, split_year):
    """
    Splits data from multiple files by time (year) and appends
    it into the FileInfoManager, holding the DataFrames in memory.
    Removes the original file after partitioning.
    """
    for filename in temporal_data:
        file_info = file_manager.get_file_info(filename)

        if file_info is not None:
            file_path = file_info.filepath
            base_path = os.path.dirname(file_path)
            df = pd.read_csv(file_path)
            v1, v2 = split_dataframe_by_time(df, split_year)

            v1_filename = modify_filename(filename, split_year - 1)
            v2_filename = filename

            v1_fakepath = os.path.join(base_path, v1_filename)
            v2_fakepath = file_path

            # Store the new DataFrames in the FileInfoManager
            file_manager.remove_file(filename)
            file_manager.add_file_by_path(v1_fakepath, v1)
            file_manager.add_file_by_path(v2_fakepath, v2)

            loguru.logger.success(f"Split {filename} into {v1_filename} and {v2_filename}")

        else:
            loguru.logger.error(f"File {filename} not found in the FileInfoManager :(")


perform_data_splitting(files_to_split, file_manager, year_to_split)
file_manager.list_files()

"""## Storing results (**Load**)
Now after splitting the data we will set the old paths to the newly generated paths (ADSDB/landing/temporal) --> (ADSDB/landing/persistent), and then save the dataframes as .csv
"""

file_manager.load_all_dataframes()

file_manager.update_filepaths(persistent_path)

def remove_previous_directory(directory):
    try:
        # Check if the directory exists
        if os.path.exists(directory) and os.path.isdir(directory):
            shutil.rmtree(directory)
            loguru.logger.success(f"Successfully removed directory: {directory}")
        else:
            loguru.logger.warning(f"Directory does not exist or is not a directory: {directory}")
    except Exception as e:
        loguru.logger.error(f"Failed to remove directory {directory}: {e}")

remove_previous_directory()

file_manager.save_all_files(persistent_path, add_timestamp=False)