# -*- coding: utf-8 -*-
"""2.formatted_zone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yHW6PoxZebgONvkl197ePIOKu66upMaX

# Formatted Zone

**Brief description**:

The purpose of the formatted zone is to gather the information from the previous ETL task (Landing Zone) and apply a set of minimal transformations that will **homogenize the data** and create a **relational canonical data source** then will be ingested by the next phase (Trusted Zone). The main purposes are:

1. Creating a relational data source
2. Normalization and standarization of the data such as dates, places and target_variables
3. Light cleaning of the data:
  - Removing duplicates
  - Removing invalid entries


---

**Input**: The main raw files of the project will be ingested from ADSDB/landing_zone/persistent

**Output**: Resulting data will be stored in .duckdb database file inside ADSDB/formatted_zone/output
> MAIN OBJECTIVE: Data quality check and standarization
"""

### Main useful libraries: loguru, ruff, pydantic, pytest, poetry, tqdm, pathlib, argparse
import os
import sys
from datetime import datetime
import glob
import re
import shutil
from dataclasses import dataclass, field
from typing import Dict, List, Optional
import pickle

!pip install fuzzywuzzy
!pip install python-Levenshtein
from fuzzywuzzy import process, fuzz

!pip install loguru
import loguru

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import duckdb

from google.colab import drive
drive.mount("/content/drive/")

formatted_zone_path = "drive/MyDrive/Assignatures/ADSDB/formatted_zone"
duckdb_path = os.path.join(formatted_zone_path, "formatted_zone.duckdb")
landing_zone_path = "drive/MyDrive/Assignatures/ADSDB/landing_zone/persistent"

"""## Initial data loading (**Extract**)

"""

@dataclass
class FileInfo:
    filename: str
    filepath: str
    df: Optional[pd.DataFrame] = field(default=None)
    source: Optional[str] = field(default=None)
    sub_source: Optional[str] = field(default=None)
    year: Optional[int] = field(default=None)

    def __repr__(self):
        return (f"FileInfo(filename='{self.filename}', filepath='{self.filepath}', "
                f"df_shape={self.df.shape if self.df is not None else None}, "
                f"source='{self.source}', sub_source='{self.sub_source}', year={self.year})")


"""
  CRUD operations defined for a set of files that are stored in multiple nested directories
  The manager provides abstractions for handling files (Even though doesn't follow RAII priniciples!!!)

"""
@dataclass
class FileInfoManager:
    directory: str
    files: Dict[str, FileInfo] = field(default_factory=dict)

    def __post_init__(self):
        self.files = self._load_csv_files(self.directory)

    """
    Recursively load all CSV files from the directory and create FileInfo objects.
    """
    def _load_csv_files(self, directory: str) -> Dict[str, FileInfo]:
        csv_file_paths = glob.glob(os.path.join(directory, '**', '*.csv'), recursive=True)
        file_info_dict = {}

        for path in csv_file_paths:
            filename = os.path.basename(path)
            file_info = self._create_file_info(filename, path)
            file_info_dict[filename] = file_info

        loguru.logger.info(f"Loaded {len(file_info_dict)} files from directory {directory}.")
        return file_info_dict

    """
    Extracts source, sub-source, and year from the filename using regex.
    Example pattern: 'source_subsource_year.csv'
    """
    def _create_file_info(self, filename: str, filepath: str, df: Optional[pd.DataFrame]=None) -> FileInfo:
        match = re.match(r'([^_]+)_([^_]+)_(\d{4})', filename)
        if match:
            source = match.group(1)
            sub_source = match.group(2)
            year = int(match.group(3))
        else:
            source = None
            sub_source = None
            year = None

        return FileInfo(filename=filename, filepath=filepath, source=source, sub_source=sub_source, year=year,df=df)

    """
    Loads a DataFrame from a CSV file if not already loaded. Returns None if file doesn't exist.
    """
    def load_dataframe(self, filename: str) -> Optional[pd.DataFrame]:
        file_info = self.files.get(filename)
        if file_info:
            if file_info.df is None:
                try:
                    file_info.df = pd.read_csv(file_info.filepath)
                    loguru.logger.success(f"DataFrame loaded for file: {filename}")
                except Exception as e:
                    loguru.logger.error(f"Failed to load DataFrame for {filename}: {e}")
                    return None
            return file_info.df
        else:
            loguru.logger.error(f"File '{filename}' not found in FileInfoManager.")
            return None

    """
      Loads all dataframes from its paths
    """
    def load_all_dataframes(self):
      for filename in self.files.keys():
        self.load_dataframe(filename)

    """
    Retrieves the FileInfo object for a given filename.
    """
    def get_file_info(self, filename: str) -> Optional[FileInfo]:
        return self.files.get(filename)

    """
    Adds a new FileInfo object to the manager.
    """
    def add_file(self, file_info: FileInfo):
        self.files[file_info.filename] = file_info
        loguru.logger.info(f"File '{file_info.filename}' added to FileInfoManager.")

    """
    Adds a new FileInfo object to the manager by filepath.
    """
    def add_file_by_path(self, filepath: str, df: Optional[pd.DataFrame] = None):
        filename = os.path.basename(filepath)
        self.files[filename] = self._create_file_info(filename, filepath, df)
        loguru.logger.info(f"File '{filename}' added to FileInfoManager.")

    """
    Removes a file from the manager by filename.
    """
    def remove_file(self, filename: str):
        if filename in self.files:
            del self.files[filename]
            loguru.logger.info(f"File '{filename}' removed from FileInfoManager.")
        else:
            loguru.logger.warning(f"Tried to remove '{filename}', but it was not found.")

    """
    Updates the file paths of each FileInfo in the FileInfoManager using the base path,
    source, sub-source, and year.

    Filepath format: base_path/source/sub_source/year.csv
    """
    def update_filepaths(self, base_path: str):
      for filename, file_info in self.files.items():
          if file_info.source and file_info.sub_source and file_info.year:
              # Construct the new file path
              new_filepath = os.path.join(base_path, file_info.source, file_info.sub_source, file_info.filename)

              # Update the FileInfo with the new path
              file_info.filepath = new_filepath
              loguru.logger.success(f"Updated {filename} -> {new_filepath}")
          else:
              loguru.logger.error(f"Skipping {filename}: source/sub_source/year information is missing")




    """
    Saves all loaded DataFrames to the specified base path.
    The directory structure is 'base_path/source/sub_source/year.csv'.
    If add_timestamp is True, appends a timestamp to the filenames.
    """
    def save_all_files(self, base_path: str, add_timestamp: bool = False):
        for file_info in self.files.values():
            if file_info.df is not None:
                # Create structured directory
                save_directory = os.path.join(base_path, file_info.source, file_info.sub_source)
                os.makedirs(save_directory, exist_ok=True)

                # Construct the filename with optional timestamp
                name, ext = os.path.splitext(file_info.filename)
                if add_timestamp:
                    timestamp = datetime.now().strftime('%Y%m%d')
                    name = f"{name}_{timestamp}"
                save_path = os.path.join(save_directory, f"{name}{ext}")

                try:
                    file_info.df.to_csv(save_path, index=False)
                    loguru.logger.success(f"File '{file_info.filename}' saved as '{name}' in '{save_directory}'")
                except Exception as e:
                    loguru.logger.error(f"Error saving '{file_info.filename}' to '{save_path}': {e}")
            else:
                loguru.logger.warning(f"File '{file_info.filename}' has no DataFrame loaded, skipping save.")

    """
    Saves a single file's DataFrame to the specified base path.
    The file will be saved as 'base_path/source/sub_source/year.csv'.
    If add_timestamp is True, appends a timestamp to the filename.
    """
    def save_file(self, filename: str, base_path: str, add_timestamp: bool = False):

        file_info = self.files.get(filename)
        if file_info and file_info.df is not None:
            save_directory = os.path.join(base_path, file_info.source, file_info.sub_source)
            os.makedirs(save_directory, exist_ok=True)

            # Construct the filename with optional timestamp
            name = f"{file_info.year}"  # Default: year.csv
            if add_timestamp:
                timestamp = datetime.now().strftime('%Y%m%d')
                name = f"{name}_{timestamp}.csv"
            else:
                name = f"{name}.csv"

            save_path = os.path.join(save_directory, name)

            try:
                file_info.df.to_csv(save_path, index=False)
                loguru.logger.success(f"File '{filename}' saved as '{name}' in '{save_directory}'")
            except Exception as e:
                loguru.logger.error(f"Error saving '{filename}' to '{save_path}': {e}")
        else:
            loguru.logger.warning(f"File '{filename}' not found or has no DataFrame loaded.")

    """
    Returns a list of all filenames
    """
    def list_files(self) -> List[str]:
        return list(self.files.keys())

    """
    Creates DuckDB tables for each CSV file in the FileInfoManager.
    The DuckDB connection is passed as an argument.
    """
    def create_duckdb_tables(self, connection: duckdb.DuckDBPyConnection):
        for filename, file_info in self.files.items():
            table_name = os.path.splitext(file_info.filename)[0]
            try:
                loguru.logger.info(f"Creating DuckDB table {table_name} from {file_info.filepath}")
                connection.execute(f"""
                    CREATE TABLE IF NOT EXISTS {table_name} AS
                    SELECT * FROM read_csv_auto('{file_info.filepath}')
                """)
                loguru.logger.success(f"Table {table_name} created successfully.")
            except Exception as e:
                loguru.logger.error(f"Error creating DuckDB table {table_name}: {e}")


    """
    Retrieves all tables from DuckDB and returns them as a dictionary of DataFrames.
    The dictionary keys are table names, and values are DataFrames.
    """
    def retrieve_all_from_duckdb(self, connection: duckdb.DuckDBPyConnection) -> Dict[str, pd.DataFrame]:
        try:
            tables = connection.execute("SHOW TABLES").fetchall()
            table_names = [table[0] for table in tables]

            dataframes = {}
            # Loop through each table and retrieve it as a DataFrame
            for table_name in table_names:
                loguru.logger.info(f"Retrieving data from DuckDB table '{table_name}'")
                df = connection.execute(f"SELECT * FROM {table_name}").fetchdf()
                dataframes[table_name] = df
                loguru.logger.success(f"Data retrieved from DuckDB table '{table_name}' successfully.")

            return dataframes
        except Exception as e:
            loguru.logger.error(f"Failed to retrieve tables from DuckDB: {e}")
            return {}

    """
    Saves an in-memory DataFrame from FileInfoManager to a DuckDB table.
    The DuckDB connection is passed as an argument.
    """
    def save_df_to_duckdb(self, connection: duckdb.DuckDBPyConnection, filename: str, table_name: str):
        file_info = self.get_file_info(filename)
        if file_info and file_info.df is not None:
            try:
                loguru.logger.info(f"Saving DataFrame to DuckDB table {table_name}")
                connection.register('df_temp', file_info.df)
                connection.execute(f"CREATE TABLE {table_name} AS SELECT * FROM df_temp")
                loguru.logger.success(f"DataFrame saved to DuckDB table {table_name} successfully.")
            except Exception as e:
                loguru.logger.error(f"Failed to save DataFrame to DuckDB table {table_name}: {e}")
        else:
            loguru.logger.error(f"No DataFrame found for {filename}.")

    """
    Searches files by optional metadata such as source, sub_source, and year.
    If a parameter is None it won't be used, therefore if all parameters are None
    it will return all files.
    """
    def search_files(self, source: Optional[str] = None, sub_source: Optional[str] = None, year: Optional[int] = None) -> List[FileInfo]:
        results = []
        for file_info in self.files.values():
            match = True
            if source is not None and file_info.source != source:
                match = False
            if sub_source is not None and file_info.sub_source != sub_source:
                match = False
            if year is not None and file_info.year != year:
                match = False
            if match:
                results.append(file_info)
        return results

    ### UPDATE FUNCTIONS

    def update_dataframe(self, filename: str, new_df: pd.DataFrame):
      file_info = self.files.get(filename)
      if file_info:
          file_info.df = new_df
          loguru.logger.info(f"DataFrame for file '{filename}' updated successfully.")
      else:
          loguru.logger.error(f"File '{filename}' not found in FileInfoManager.")

    def update_file_info(self, filename: str, source: Optional[str] = None, sub_source: Optional[str] = None, year: Optional[int] = None):
      file_info = self.files.get(filename)
      if file_info:
          if source is not None:
              file_info.source = source
          if sub_source is not None:
              file_info.sub_source = sub_source
          if year is not None:
              file_info.year = year
          loguru.logger.info(f"FileInfo for '{filename}' updated successfully.")
      else:
          loguru.logger.error(f"File '{filename}' not found in FileInfoManager.")




# Example usage
file_manager = FileInfoManager(directory=landing_zone_path)
file_manager.list_files()
file_manager.load_all_dataframes()

"""## Standarization and transformations (**Transform**)

1. Standarization of the time data of portaldades source (Time is treated as multiple variables rather than just one)

2. Look for possible errors or mismatches in the districts of barcelona to integrate opendata + portaldades data (Maybe a unifying dictionary will be used)

3. Handling the tricky demographics table

### 1. Standardization

This section will cover time standarization in the **portaldades** data. After standarizing this time-series data to a yearly format we will be able to have consistency across all the data.

We identified several issues in the pivoted DataFrames:

- **Duplicate Columns**: The "lloguer recompte" DataFrame contains duplicate columns for the year 2000 ("2000" and "2000.1"). This needs to be corrected.

To streamline the analysis, we will retain only the latest data from each year for two reasons:

1. **Granularity**: High granularity has minimal impact on the analysis.
2. **Consistency**: Aligning with other sources that use an annual format simplifies standardization.
"""

def convert_quarter_str_to_date(quarter_str):
    pattern = r"(\d)[nrt]\str\s(\d{4})"
    matches = re.match(pattern, quarter_str)

    if matches:
        quarter = int(matches.group(1))
        year = int(matches.group(2))

        # Map quarter into specific dates
        quarters_to_dates = {
            1: pd.Timestamp(f"{year}-01-01"),
            2: pd.Timestamp(f"{year}-04-01"),
            3: pd.Timestamp(f"{year}-07-01"),
            4: pd.Timestamp(f"{year}-10-01")
        }
        return quarters_to_dates[quarter]
    return None

def standarize_portaldades_prices(df: pd.DataFrame) -> pd.DataFrame:
    df = pd.melt(
        df,
        id_vars=["Territori", "Tipus de territori"],
        var_name="Temps",
        value_name="Preu mitja lloguer"
    )

    df["Temps"] = df["Temps"].apply(convert_quarter_str_to_date)

    # Keep the maximum date for each year and group
    df["Year"] = df["Temps"].dt.year
    df = df.loc[df.groupby(["Year", "Territori", "Tipus de territori"])["Temps"].idxmax()]

    # Drop the 'Temps' column and keep only 'Year'
    df = df[["Territori", "Tipus de territori", "Year", "Preu mitja lloguer"]]
    df.rename(columns={"Year": "Temps"}, inplace=True)
    df.reset_index(drop=True, inplace=True)
    return df

def standarize_portaldades_counts(df: pd.DataFrame) -> pd.DataFrame:
    df = pd.melt(
        df,
        id_vars=["Territori", "Tipus de territori"],
        var_name="Temps",
        value_name="Nombre de lloguers"
    )
    df["Temps"] = pd.to_datetime(df["Temps"], format="%Y").dt.year
    df.reset_index(drop=True, inplace=True)
    return df

def etl_standarize_time_portaldades(file_manager: FileInfoManager) -> None:
  #Extract
  count_files = file_manager.search_files(source="portaldades", sub_source="lloguerrecompte")
  price_files = file_manager.search_files(source="portaldades", sub_source="lloguerpreus")
  count_dfs = {file.filename: file.df for file in count_files if file.df is not None}
  price_dfs = {file.filename: file.df for file in price_files if file.df is not None}

  #Transform 1
  count_dfs["portaldades_lloguerrecompte_2022.csv"].drop(columns=["2000.1"], inplace=True, errors="ignore")
  #Transform 2
  for filename in count_dfs.keys():
    count_dfs[filename] = standarize_portaldades_counts(count_dfs[filename])

  for filename in price_dfs.keys():
    price_dfs[filename] = standarize_portaldades_prices(price_dfs[filename])
  #Load
  dfs = {**count_dfs, **price_dfs}
  for filename in dfs.keys():
    file_manager.update_dataframe(filename, dfs[filename])

  loguru.logger.success("Date standardization (Portaldades) process completed successfully.")


etl_standarize_time_portaldades(file_manager)

def etl_standarize_time_opendata(file_manager: FileInfoManager, df_names: List[str], col_name: str) -> None:
    # Extract
    opendata_files = file_manager.search_files(source="opendata")
    opendata_dfs = {file.filename: file.df for file in opendata_files if file.df is not None and file.filename in df_names}

    # Transform 1: Standardize date into year
    target_col_name = "Any"

    for filename, df in opendata_dfs.items():
        if col_name in df.columns:
            # Convert the date column to datetime format and extract the year
            df[target_col_name] = pd.to_datetime(df[col_name], errors='coerce').dt.year
            df.drop(columns=[col_name], inplace=False)

            loguru.logger.info(f"Transformed '{col_name}' into '{target_col_name}' in '{filename}'.")
        else:
            loguru.logger.warning(f"Column '{col_name}' not found in '{filename}'.")

        # Load: Update the modified DataFrame back to the file manager
        file_manager.update_dataframe(filename, df)


    loguru.logger.success("Date standardization (Opendata) process completed successfully.")

etl_standarize_time_opendata(file_manager, df_names=["opendata_demografia_2020.csv", "opendata_demografia_2021.csv"], col_name="Data_Referencia")

"""### 2. Unification

Now we need a way to be able to unificate similar but distinct geographical information that is present in all the different data sources from Barcelona. Both data sources have different levels of aggregation in this kind of data:  

- **Opendata**: Districte / Barri / Secció Censal (*Granularity sorted asc*)

- **Portaldades**: Comunitat Autonoma / Ambit Funcional Territorial / Municipi / Districte / Barri (*Granularity sorted asc*)

Therefore we will be able to join these tables by time and and location (Districte or Barri). However there's a wide range of problems involving the spatial component of this data:

1. **Unified Geographical ID's**: Moreover, we would need an unified way of uniquely identifying geographical areas/places, therefore we will create a **concatenated unique id**. Each exact position of the string will match a level by a regex. Examples: "Catalunya" -> "01", "Barcelona": "0102",..., "El Raval:  "01020103"...

2. **Handling Inconsistent Names**: Both datasets have really similar information of places but some of the examples don't match (i.e "El Raval" ≠ "Raval"). To handle this issue we will use fuzzy string matching with Levehnstein distance algorithms.

---

We need to ensure these string values match each other, through unification mappings.


1. **Place-to-ID mapping**: This dictionary will allow us to perform translations from place to numerical ID, such as "El Raval" -> 01020103. This numerical ID follows a hierarchical concatenation strategy, thus can be expanded by changing the regex to match the exact positions and the geographical tree. This mapping will be a union of all the levels to avoid recursive searching. **This provides a fast and easy look-up mechanism for translating a place to its unique ID.** This structure can be derived from the geographical tree to avoid hardcoding and rigidity in the data pipeline.

2. **Geographical tree**: This Dictionary-Tree structure contains all the hierarchical information of the places, sorted by granularity, starting by a root node which represents "Catalunya", the only node of the first level which represents "Comunitat Autonoma". **This provides a fast look-up mechanism for hierarchical missing information**. This geographical tree will automatically generate the Place-to-ID mapping, therefore this will be an important data source (Despite being synthetic) that will allow for flexibility in the pipeline.  


> **ASSUMPTION**: We just care about information about the metropolitan area of Barcelona, (Catalunya) therefore we will ignore other "Ambits funcionals territorials" and other "Comunitats Autonomes".

"""

#Alternatively one could use the serialized .pickle

#Visualize the tree and serialize it to use it as a data source
geographical_tree = {
    "Catalunya": {
        "Metropolità de Barcelona": {
            "Barcelona": {
                'Ciutat Vella': {
                    'el Raval': {1: {}, 2: {}, 3: {}, 4: {},5: {}, 6: {}, 7: {}, 8: {}, 9: {}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {}, 16: {}, 17: {}, 18: {}, 19: {}, 20: {}, 21: {}},
                    'el Barri Gòtic': {22: {}, 23: {}, 25: {}, 26: {}, 27: {}, 28: {}, 29: {}, 30: {}, 31: {}},
                    'la Barceloneta': {32: {}, 33: {}, 34: {}, 35: {}, 36: {}, 37: {}, 38: {}, 39: {}, 40: {}, 41: {}, 42: {}},
                    'Sant Pere, Santa Caterina i la Ribera': {43: {}, 44: {}, 45: {}, 46: {}, 47: {}, 48: {}, 49: {}, 50: {}, 51: {}, 52: {}, 53: {}, 54: {}, 55: {}},
                },
                "L'Eixample": {
                    'el Fort Pienc': {1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {}, 9: {}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {}, 16: {}, 17: {}, 18: {}, 19: {}, 20: {}},
                    'la Sagrada Família': {21: {}, 22: {}, 23: {}, 24: {}, 25: {}, 26: {}, 27: {}, 28: {}, 29: {}, 30: {}, 31: {}, 32: {}, 33: {}, 34: {}, 35: {}, 36: {}, 37: {}, 38: {}, 39: {}, 40: {}, 41: {}, 42: {}, 43: {}, 44: {}, 45: {}, 46: {}, 47: {}, 48: {}, 49: {}, 50: {}, 51: {}, 52: {}, 53:  {}, 54: {}},
                    "la Dreta de l'Eixample": {55: {}, 56: {}, 57: {}, 58: {}, 59: {}, 60: {}, 61: {}, 62: {}, 63: {}, 64: {}, 65: {}, 66: {}, 67: {}, 68: {}, 69: {}, 70: {}, 71: {}, 72: {}, 73: {}, 74: {}, 75: {}, 76: {}, 77: {}, 78: {}, 79: {}, 80: {}, 81: {}, 82: {}, 83: {}},
                    "l'Antiga Esquerra de l'Eixample": {84: {}, 85: {}, 86: {}, 87: {}, 88: {}, 89: {}, 90: {}, 91: {}, 92: {}, 93: {}, 94: {}, 95: {}, 96: {}, 97: {}, 98: {}, 99: {}, 100: {}, 101: {}, 102: {}, 103: {}, 104: {}, 105: {}, 106: {}, 107: {}, 108: {}, 109: {}},
                    "la Nova Esquerra de l'Eixample": {110: {}, 111: {}, 112: {}, 113: {}, 114: {}, 115: {}, 116: {}, 117: {}, 118: {}, 119: {}, 120: {}, 121: {}, 122: {}, 123: {}, 124: {}, 125: {}, 126: {}, 127: {}, 128: {}, 129: {}, 130: {}, 131: {}, 132: {}, 133: {}, 134: {}, 135: {}, 136: {}, 137: {}, 138: {}, 139: {}, 140: {}, 141: {}, 142: {}, 143: {}, 144: {}, 145: {}, 146: {}, 147: {}, 148: {}, 149: {}},
                    'Sant Antoni': {150: {}, 151: {}, 152: {}, 153: {}, 154: {}, 155: {}, 156: {}, 157: {}, 158: {}, 159: {}, 160: {}, 161: {}, 162: {}, 163: {}, 164: {}, 165: {}, 166: {}, 167: {}, 168: {}, 169: {}, 170: {}, 171: {}, 172: {}, 173: {}},
                },
                'Sants-Montjuïc': {
                    'el Poble Sec': {1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {}, 9: {}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {}, 16: {}, 17: {}, 18: {}, 19: {}, 20: {}, 21: {}, 22: {}, 23: {}, 24: {}},
                    'la Marina del Prat Vermell': {25: {}},
                    'la Marina de Port': {26: {}, 27: {}, 28: {}, 29: {}, 30: {}, 31: {}, 32: {}, 33: {}, 34: {}, 35: {}, 36: {}, 37: {}, 38: {}, 39: {}, 211: {}, 212: {}, 213: {}},
                    'la Font de la Guatlla': {40: {}, 41: {}, 42: {}, 43: {}, 44: {}, 45: {}, 46: {}},
                    'Hostafrancs': {47: {}, 48: {}, 49: {}, 50: {}, 51: {}, 52: {}, 53: {}, 54: {}, 55: {}, 56: {}},
                    'la Bordeta': {57: {}, 58: {}, 59: {}, 60: {}, 61: {}, 62: {}, 63: {}, 64: {}, 65: {}, 66: {}, 67: {}, 68: {}, 69: {}},
                    'Sants-Badal': {70: {}, 71: {}, 72: {}, 73: {}, 74: {}, 75: {}, 76: {}, 77: {}, 78: {}, 79: {}, 80: {}, 81: {}, 82: {}, 83: {}, 84: {}, 85: {}},
                    'Sants': {86: {}, 87: {}, 88: {}, 89: {}, 90: {}, 91: {}, 92: {}, 93: {}, 94: {}, 95: {}, 96: {}, 97: {}, 98: {}, 99: {}, 100: {}, 101: {}, 102: {}, 103: {}, 104: {}, 105: {}, 106: {}, 107: {}, 108: {}, 109: {}, 110: {}, 111: {}, 112: {}, 113: {}, 114: {}},
                },
                'Les Corts': {
                    'les Corts': {1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {}, 9: {}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {}, 16: {}, 17: {}, 18: {}, 19: {}, 20: {}, 21: {}, 22: {}, 23: {}, 24: {}, 25: {}, 26: {}, 27: {}, 28: {}, 29: {}, 30: {}, 31: {}, 32: {}, 33: {}, 34: {}, 35: {}},
                    'la Maternitat i Sant Ramon': {36: {}, 37: {}, 38: {}, 39: {}, 40: {}, 41: {}, 42: {}, 43: {}, 44: {}, 45: {}, 46: {}, 47: {}, 48: {}, 49: {}, 84: {}},
                    'Pedralbes': {50: {}, 51: {}, 52: {}, 53: {}, 54: {}, 55: {}, 56: {}},
                },
                'Sarrià-Sant Gervasi': {
                    'Vallvidrera, el Tibidabo i les Planes': {1: {}, 2: {}, 3: {}},
                    'Sarrià': {4: {}, 5: {}, 6: {}, 7: {}, 8: {}, 9: {}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {}, 16: {}, 17: {}, 18: {}, 19: {}},
                    'les Tres Torres': {20: {}, 21: {}, 22: {}, 23: {}, 24: {}, 25: {}, 26: {}, 27: {}, 28: {}, 29: {}, 30: {}},
                    'Sant Gervasi- la Bonanova': {31: {}, 32: {}, 33: {}, 34: {}, 35: {}, 36: {}, 37: {}, 38: {}, 39: {}, 40: {}, 41: {}, 42: {}, 43: {}, 44: {}, 45: {}, 46: {}, 47: {}, 48: {}},
                    'Sant Gervasi- Galvany': {49: {}, 50: {}, 51: {}, 52: {}, 53: {}, 54: {}, 55: {}, 56: {}, 57: {}, 58: {}, 59: {}, 60: {}, 61: {}, 62: {}, 63: {}, 64: {}, 65: {}, 66: {}, 67: {}, 68: {}, 69: {}, 70: {}, 71: {}, 72: {}, 73: {}, 74: {}, 75: {}, 76: {}, 77: {}, 78: {}, 79: {}},
                    'el Putxet i el Farró': {80: {}, 81: {}, 82: {}, 83: {}, 84: {}, 85: {}, 86: {}, 87: {}, 88: {}, 89: {}, 90: {}, 91: {}, 92: {}, 93: {}, 94: {}, 95: {}, 96: {}, 97: {}, 98: {}},
                },
                'Gràcia': {
                    'Vallcarca i els Penitents': {1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {}, 9: {}, 10: {}, 11: {}, 12: {}},
                    'el Coll': {13: {}, 14: {}, 15: {}, 16: {}, 17: {}},
                    'la Salut': {18: {}, 19: {}, 20: {}, 21: {}, 22: {}, 23: {}, 24: {}, 25: {}, 26: {}, 27: {}},
                    'la Vila de Gràcia': {28: {}, 29: {}, 30: {}, 31: {}, 32: {}, 33: {}, 34: {}, 35: {}, 36: {}, 37: {}, 38: {}, 39: {}, 40: {}, 41: {}, 42: {}, 43: {}, 44: {}, 45: {}, 46: {}, 47: {}, 48: {}, 49: {}, 50: {}, 51: {}, 52: {}, 53: {}, 54: {}, 55: {}, 56: {}, 57: {}, 58: {}, 59: {}, 60: {}, 61: {}, 62: {}, 63: {}},
                    "el Camp d'en Grassot i Gràcia Nova": {64: {}, 65: {}, 66: {}, 67: {}, 68: {}, 69: {}, 70: {}, 71: {}, 72: {}, 73: {}, 74: {}, 75: {}, 76: {}, 77: {}, 78: {}, 79: {}, 80: {}, 81: {}, 82: {}, 83: {}, 84: {}, 85: {}, 86: {}, 87: {}, 88: {}},
                },
                'Horta-Guinardó': {
                    'el Baix Guinardó': {1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {}, 9: {}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {}, 16: {}, 17: {}, 18: {}, 19: {}},
                    'Can Baró': {20: {}, 21: {}, 22: {}, 23: {}, 24: {}, 25: {}, 26: {}},
                    'el Guinardó': {27: {}, 28: {}, 29: {}, 30: {}, 31: {}, 32: {}, 33: {}, 34: {}, 35: {}, 36: {}, 37: {}, 38: {}, 39: {}, 40: {}, 41: {}, 42: {}, 43: {}, 44: {}, 45: {}, 46: {}, 47: {}, 48: {}, 49: {}, 50: {}, 51: {}},
                    "la Font d'en Fargues": {52: {}, 53: {}, 54: {}, 55: {}, 56: {}, 57: {}, 58: {}},
                    'el Carmel': {59: {}, 60: {}, 61: {}, 62: {}, 63: {}, 64: {}, 65: {}, 66: {}, 67: {}, 68: {}, 69: {}, 70: {}, 71: {}, 72: {}, 73: {}, 74: {}, 75: {}, 76: {}, 77: {}, 78: {}, 79: {}, 80: {}},
                    'la Teixonera': {81: {}, 82: {}, 83: {}, 84: {}, 85: {}, 86: {}, 87: {}, 88: {}},
                    'Sant Genís dels Agudells': {89: {}, 90: {}, 91: {}, 92: {}, 93: {}},
                    'Montbau': {94: {}, 95: {}, 96: {}, 97: {}},
                    "la Vall d'Hebron": {98: {}, 99: {}, 100: {}, 101: {}},
                    'la Clota': {102: {}},
                    'Horta': {103: {}, 104: {}, 105: {}, 106: {}, 107: {}, 108: {}, 109: {}, 110: {}, 111: {}, 112: {}, 113: {}, 114: {}, 115: {}, 116: {}, 117: {}, 118: {}, 119: {}, 120: {}, 121: {}, 122: {}, 123: {}},
                },
                'Nou Barris': {
                    'Vilapicina i la Torre Llobeta': {1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {}, 9: {}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {}, 16: {}, 17: {}, 18: {}, 19: {}},
                    'Porta': {20: {}, 21: {}, 22: {}, 23: {}, 24: {}, 25: {}, 26: {}, 27: {}, 28: {}, 29: {}, 30: {}, 31: {}, 32: {}, 33: {}, 34: {}, 35: {}, 36: {}, 37: {}},
                    'el Turó de la Peira': {38: {}, 39: {}, 40: {}, 41: {}, 42: {}, 43: {}, 44: {}, 45: {}, 46: {}, 47: {}},
                    'Can Peguera': {48: {}, 49: {}},
                    'la Guineueta': {50: {}, 51: {}, 52: {}, 53: {}, 54: {}, 55: {}, 56: {}, 57: {}, 58: {}, 59: {}, 215: {}},
                    'Canyelles': {60: {}, 61: {}, 62: {}, 63: {}, 64: {}},
                    'les Roquetes': {65: {}, 66: {}, 67: {}, 68: {}, 69: {}, 70: {}, 71: {}, 72: {}, 73: {}, 74: {}},
                    'Verdun': {75: {}, 76: {}, 77: {}, 78: {}, 79: {}, 80: {}, 81: {}, 82: {}},
                    # 83 to 102
                    'la Prosperitat': {83: {}, 84: {}, 85: {}, 86: {}, 87: {}, 88: {}, 89: {}, 90: {}, 91: {}, 92: {}, 93: {}, 94: {}, 95: {}, 96: {}, 97: {}, 98: {}, 99: {}, 100: {}, 101: {}, 102: {}},
                    'la Trinitat Nova': {103: {}, 104: {}, 105: {}, 106: {}, 107: {}},
                    'Torre Baró': {108: {}, 109: {}},
                    'Ciutat Meridiana': {110: {}, 111: {}, 112: {}, 113: {}, 114: {}, 115: {}},
                    'Vallbona': {116: {}}
                },
                'Sant Andreu': {
                    'la Trinitat Vella': {1: {}, 2: {}, 3: {}, 4: {}, 5: {}},
                    'Baró de Viver': {6: {}, 7: {}},
                    'el Bon Pastor': {8: {}, 9: {}, 10: {}, 11: {}, 13: {}, 14: {}, 15: {}},
                    'Sant Andreu': {16: {}, 17: {}, 18: {}, 19: {}, 20: {}, 21: {}, 22: {}, 23: {}, 24: {}, 25: {}, 26: {}, 27: {}, 28: {}, 29: {}, 30: {}, 31: {}, 32: {}, 33: {}, 34: {}, 35: {}, 36: {}, 37: {}, 38: {}, 39: {}, 40: {}, 41: {}, 42: {}, 43: {}, 44: {}, 45: {}, 46: {}, 47: {}, 48: {}, 49: {}, 50: {}, 51: {}, 52: {}, 53: {}, 54: {}},
                    'la Sagrera': {55: {}, 56: {}, 57: {}, 58: {}, 59: {}, 60: {}, 61: {}, 62: {}, 63: {}, 64: {}, 65: {}, 66: {}, 67: {}, 68: {}, 69: {}, 70: {}, 71: {}, 72: {}, 73: {}, 156: {}},
                    'el Congrés i els Indians': {74: {}, 75: {}, 76: {}, 77: {}, 78: {}, 79: {}, 80: {}, 81: {}, 82: {}},
                    'Navas': {83: {}, 84: {}, 85: {}, 86: {}, 87: {}, 88: {}, 89: {}, 90: {}, 91: {}, 92: {}, 93: {}, 94: {}, 95: {}},
                },
                'Sant Martí': {
                    "el Camp de l'Arpa del Clot": {1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {}, 9: {}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {}, 16: {}, 17: {}, 18: {}, 19: {}, 20: {}, 21: {}, 22: {}, 23: {}, 24: {}, 25: {}},
                    'el Clot': {26: {}, 27: {}, 28: {}, 29: {}, 30: {}, 31: {}, 32: {}, 33: {}, 34: {}, 35: {}, 36: {}, 37: {}, 38: {}, 39: {}, 40: {}, 41: {}, 243: {}},
                    'el Parc i la Llacuna del Poblenou': {42: {}, 43: {}, 44: {}, 45: {}, 46: {}, 47: {}, 48: {}, 49: {}, 50: {}},
                    'la Vila Olímpica del Poblenou': {51: {}, 52: {}, 53: {}, 54: {}, 55: {}},
                    'el Poblenou': {56: {}, 57: {}, 58: {}, 59: {}, 60: {}, 61: {}, 62: {}, 63: {}, 64: {}, 65: {}, 66: {}, 67: {}, 68: {}, 69: {}, 70: {}, 71: {}, 72: {}, 73: {}, 74: {}, 75: {}},
                    'Diagonal Mar i el Front Marítim del Poblenou': {76: {}, 77: {}, 78: {}, 79: {}, 80: {}, 235: {}, 236: {}},
                    'el Besòs i el Maresme': {81: {}, 82: {}, 83: {}, 84: {}, 85: {}, 86: {}, 87: {}, 88: {}, 89: {}, 90: {}, 91: {}, 92: {}, 93: {}},
                    'Provençals del Poblenou': {94: {}, 95: {}, 96: {}, 97: {}, 98: {}, 99: {}, 100: {}, 101: {}, 102: {}, 103: {}, 104: {}, 237: {}},
                    'Sant Martí de Provençals': {105: {}, 106: {}, 107: {}, 108: {}, 109: {}, 110: {}, 111: {}, 112: {}, 113: {}, 114: {}, 115: {}, 116: {}, 117: {}, 118: {}, 119: {}, 120: {}, 121: {}, 122: {}},
                    'la Verneda i la Pau': {123: {}, 124: {}, 125: {}, 126: {}, 127: {}, 128: {}, 129: {}, 130: {}, 131: {}, 132: {}, 133: {}, 134: {}, 135: {}, 136: {}, 137: {}, 138: {}, 139: {}, 140: {}, 141: {}, 142: {}, 143: {}},
                }

            }
        },
        #Other ambit funcionals
        "Comarques Gironines": {},
        "Alt Pirineu i Aran": {},
        "Comarques Centrals": {},
        "Ponent": {},
        "Penedès": {},
        "Camp de Tarragona": {},
        "Terres de l'Ebre": {},
    }
  }

def generate_hierarchical_flattened_ids(location_dict, current_id=""):
    """
    Recursively generate hierarchical IDs with 3 digits for each location level, using the hierarchical ID itself as the unique identifier.

    Parameters:
    - location_dict (dict): Nested dictionary representing the location hierarchy.
    - current_id (str): The current hierarchical ID (starts as an empty string).

    Returns:
    dict: Dictionary mapping location names to their hierarchical IDs.
    """
    id_dict = {}
    level_counter = 1

    for location, sublocations in location_dict.items():
        # Generate a unique ID by appending a three-digit counter to the current_id
        new_id = f"{current_id}{level_counter:03d}"

        # Check if the location is a "Secció Censal" (numeric key)
        if isinstance(location, int):
            location_name = f"Secció Censal {new_id}"
        else:
            location_name = location

        id_dict[location_name] = new_id

        # Recursively generate IDs for the sublocations
        if sublocations:
            sublocation_ids = generate_hierarchical_flattened_ids(sublocations, new_id)
            id_dict.update(sublocation_ids)

        level_counter += 1

    return id_dict


geographical_ids = generate_hierarchical_flattened_ids(geographical_tree, "")

"""### 2.1 Handling Inconsistent Names"""

def drop_columns(df: pd.DataFrame, columns_to_drop: list) -> pd.DataFrame:
    # Ensure that only existing columns are attempted to be dropped
    columns_to_drop = [col for col in columns_to_drop if col in df.columns]
    return df.drop(columns=columns_to_drop)


def fuzzy_match_places(place, place_list, threshold=50):
    """
    Matches a place name to a list of possible places using fuzzy matching.
    """
    match, score = process.extractOne(place, place_list, scorer=fuzz.token_sort_ratio)
    if score >= threshold:
        return match
    return None

def fuzzy_match_from_dict(place, place_dict, threshold=50):
    """
    Matches a place name to the keys of a dictionary using fuzzy matching.
    Returns the best match and its associated ID.
    """
    place_list = list(place_dict.keys())
    match, score = process.extractOne(place, place_list, scorer=fuzz.token_sort_ratio)
    if score >= threshold:
        return match, place_dict[match]
    return None, None


def correct_portaldades_places(df, geographical_ids):
    """
    Correct the 'Nom_Districte', 'Nom_Barri', and optionally 'Seccio_Censal' columns using the geographical_ids dictionary.
    The dictionary provides standardized names and unique hierarchical IDs.
    """
    # Define the fields we want to correct
    fields = [
        ('Nom_Districte', 'Id_Districte'),
        ('Nom_Barri', 'Id_Barri'),
    ]

    # Check if 'Seccio_Censal' exists and add it to the fields if it does
    if 'Seccio_Censal' in df.columns:
        fields.append(('Seccio_Censal', 'Id_Seccio_Censal'))

    new_values = {field: [] for field, _ in fields}
    new_ids = {id_field: [] for _, id_field in fields}

    for idx, row in df.iterrows():
        for name, id_name in fields:
            # Special handling for 'Seccio_Censal' to format it correctly
            original_value = row[name] if name != 'Seccio_Censal' else f"Secció Censal {row['Seccio_Censal']}"

            # Fuzzy match the current field
            match, id_value = fuzzy_match_from_dict(original_value, geographical_ids)

            new_values[name].append(match if match else original_value)
            new_ids[id_name].append(id_value if match else None)

    # Update
    for name in new_values.keys():
        df[name] = new_values[name]
    for id_name in new_ids.keys():
        df[id_name] = new_ids[id_name]

    return df

# Sanity check to ensure there are no null values
def etl_correct_places_opendata(file_manager: FileInfoManager, geographical_ids: Dict) -> None:
    # Extract
    opendata_files = file_manager.search_files(source="opendata")
    opendata_dfs = {file.filename: file.df for file in opendata_files if file.df is not None}

    # Sanity check: Ensure 'Id_Barri' is not null for all DataFrames
    for filename, df in opendata_dfs.items():
        if "Id_Barri" in df.columns:
            non_null_count = df["Id_Barri"].notnull().sum()
            total_count = len(df)
            if non_null_count != total_count:
                loguru.logger.warning(f"Sanity check failed for {filename}: "
                               f"{non_null_count} non-null 'Id_Barri' values out of {total_count}")

    # Transform 1: Drop specified columns
    opendata_dfs = {file: drop_columns(df, ["Codi_Districte", "Codi_Barri"]) for file, df in opendata_dfs.items()}

    # Transform 2: Correct place names and IDs
    opendata_dfs = {file: correct_portaldades_places(df, geographical_ids) for file, df in opendata_dfs.items()}

    # Load
    for filename, df in opendata_dfs.items():
        file_manager.update_dataframe(filename, df)

    loguru.logger.success("Places standardization (Opendata) process completed successfully.")

etl_correct_places_opendata(file_manager, geographical_ids)

"""### 2.2 Unfied Geographical ID's"""

def drop_columns(df: pd.DataFrame, columns_to_drop: list) -> pd.DataFrame:
    # Ensure that only existing columns are attempted to be dropped
    columns_to_drop = [col for col in columns_to_drop if col in df.columns]
    return df.drop(columns=columns_to_drop)


def pivot_portaldades_table(df, value_column):
    # Pivot 'Territori' to create columns for each 'Tipus de territori'
    territori_pivoted = df.pivot(
        index=["Temps", "Territori"],
        columns="Tipus de territori",
        values="Territori"
    )

    # Pivot the specified metric column (e.g., 'Nombre de lloguers' or 'Preu mitja lloguer')
    metric_pivoted = df.pivot(
        index=["Temps", "Territori"],
        columns="Tipus de territori",
        values=value_column
    )

    # Concatenate the two pivoted tables along columns
    pivoted_df = pd.concat([territori_pivoted, metric_pivoted.add_prefix(f'{value_column}_')], axis=1)

    # Consolidate the metric columns into a single column
    pivoted_df[value_column] = pivoted_df.filter(regex=f'^{value_column}_').bfill(axis=1).iloc[:, 0]

    # Drop the original prefixed metric columns now that they’re consolidated
    pivoted_df.drop(columns=pivoted_df.filter(regex=f'^{value_column}_').columns, inplace=True)

    # Flatten the columns
    pivoted_df.columns = [f'{col}' if isinstance(col, str) else f'{col[1]}' for col in pivoted_df.columns]
    pivoted_df.reset_index(inplace=True)

    return pivoted_df

def etl_pivot_portaldades_tables(file_manager: FileInfoManager) -> None:
  #Extract
  count_files = file_manager.search_files(source="portaldades", sub_source="lloguerrecompte")
  price_files = file_manager.search_files(source="portaldades", sub_source="lloguerpreus")
  count_dfs = {file.filename: file.df for file in count_files if file.df is not None}
  price_dfs = {file.filename: file.df for file in price_files if file.df is not None}

  #Transform 1: Pivot the tables to a wide format
  pivoted_count_dfs = {file: pivot_portaldades_table(df, value_column="Nombre de lloguers") for file, df in count_dfs.items()}
  pivoted_price_dfs = {file: pivot_portaldades_table(df, value_column="Preu mitja lloguer") for file, df in price_dfs.items()}
  pivoted_dfs = {**pivoted_count_dfs, **pivoted_price_dfs}

  #Transform 2: Drop useless column "-"
  pivoted_dfs = {file: drop_columns(df, ["-"]) for file, df in pivoted_dfs.items()}

  #Load
  for filename, df in pivoted_dfs.items():
    file_manager.update_dataframe(filename, df)
  loguru.logger.success("Pivoting (Portaldades) process completed successfully")

etl_pivot_portaldades_tables(file_manager)

def correct_portaldades_places(df: pd.DataFrame, geographical_ids: Dict) -> pd.DataFrame:
    """
    Corrects place names and assigns unique IDs in a portaldades DataFrame
    using the geographical_ids dictionary, with special handling for NaN values.
    """
    # Define the fields we want to correct and match with their unique IDs
    fields = [
        ('Territori', 'Id_Territori'),
        ('Barri', 'Id_Barri'),
        ('Comunitat Autònoma', 'Id_Comunitat'),
        ('Districte', 'Id_Districte'),
        ('Municipi', 'Id_Municipi')
    ]

    # Prepare new columns to store the corrected values and IDs
    new_values = {field: [] for field, _ in fields}
    new_ids = {id_field: [] for _, id_field in fields}

    # Apply fuzzy matching for each row in each specified field
    for idx, row in df.iterrows():
        for name, id_name in fields:
            original_value = row[name]

            if pd.isna(original_value):
                # If the value is NaN, skip matching and set both corrected name and ID to None
                new_values[name].append(None)
                new_ids[id_name].append(None)
            else:
                # Perform fuzzy matching only if original_value is not NaN
                match, id_value = fuzzy_match_from_dict(original_value, geographical_ids)

                # Append the matched value or original if no match
                new_values[name].append(match if match else original_value)
                new_ids[id_name].append(id_value if match else None)

    # Update DataFrame with corrected names and unique IDs
    for name in new_values.keys():
        df[name] = new_values[name]
    for id_name in new_ids.keys():
        df[id_name] = new_ids[id_name]

    return df


def etl_correct_places_portaldades(file_manager: FileInfoManager, geographical_ids: Dict) -> None:
    # Extract
    portaldades_files = file_manager.search_files(source="portaldades")
    portaldades_dfs = {file.filename: file.df for file in portaldades_files if file.df is not None}

    # Transform: Correct place names and assign IDs
    portaldades_dfs = {file: correct_portaldades_places(df, geographical_ids) for file, df in portaldades_dfs.items()}


    # Sanity check: Ensure 'Id_Barri' is not null
    for filename, df in portaldades_dfs.items():
        if "Id_Barri" in df.columns:
            non_null_count = df["Id_Barri"].notnull().sum()
            total_count = len(df)
            if non_null_count != total_count:
                loguru.logger.warning(f"Sanity check for {filename}: "
                               f"{non_null_count} non-null 'Id_Barri' values out of {total_count}")

    # Unify NA's

    # Load
    for filename, df in portaldades_dfs.items():
        file_manager.update_dataframe(filename, df)

    loguru.logger.success("Places standardization (Portaldades) process completed successfully.")



etl_correct_places_portaldades(file_manager, geographical_ids)

def etl_correct_target_variables_portaldades(file_manager: FileInfoManager) -> None:
    """
    ETL function to handle null values and convert target columns to numeric
    for portaldades DataFrames.
    """
    # Extract
    portaldades_files = file_manager.search_files(source="portaldades")
    portaldades_dfs = {file.filename: file.df for file in portaldades_files if file.df is not None}

    # Transform: Handle "-" as null values and convert columns to numeric
    target_columns = ["Nombre de lloguers", "Preu mitja lloguer"]
    for filename, df in portaldades_dfs.items():
        for col in target_columns:
            if col in df.columns:
                # Replace "-" with NaN, then convert to numeric
                df[col] = pd.to_numeric(df[col].replace("-", pd.NA), errors='coerce')

    # Load
    for filename, df in portaldades_dfs.items():
        file_manager.update_dataframe(filename, df)

    loguru.logger.success("Target variable correction (Portaldades) process completed successfully.")

# Example call
etl_correct_target_variables_portaldades(file_manager)

"""## Create a DuckDB relational database (**Load**)"""

connection = duckdb.connect(database=duckdb_path, read_only=False)
file_manager.create_duckdb_tables(connection)
connection.query("SHOW TABLES")

connection.close()